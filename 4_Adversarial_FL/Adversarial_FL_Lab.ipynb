{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b66f6d",
   "metadata": {},
   "source": [
    "# Adversarial Federated Learning Lab\n",
    "\n",
    "Welcome to Module 4's hands-on lab. You'll orchestrate a surrogate-driven poisoning attack inside this notebook by wiring together the same building blocks the adversary would use in code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb59bb",
   "metadata": {},
   "source": [
    "> Use this lab as a guided worksheet: read each section, execute the code, and jot down observations. Feel free to duplicate the notebook if you want to keep notes or alternative attack settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91493e0",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "We start by locating the project root so we can import the Module 4 package. Because the package name begins with a number we rely on `importlib.import_module` instead of the usual `from ... import ...` form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d3a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "from importlib import import_module\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if not (PROJECT_ROOT / \"4_Adversarial_FL\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "PACKAGE_ROOT = PROJECT_ROOT / \"4_Adversarial_FL\"\n",
    "\n",
    "if not PACKAGE_ROOT.exists():\n",
    "    raise RuntimeError(\"Run this notebook from the repo root or the 4_Adversarial_FL directory.\")\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "load_data_module = import_module(\"4_Adversarial_FL.load_data_for_clients\")\n",
    "client_module = import_module(\"4_Adversarial_FL.client\")\n",
    "malicious_module = import_module(\"4_Adversarial_FL.malicious_client\")\n",
    "model_module = import_module(\"4_Adversarial_FL.model\")\n",
    "utils_module = import_module(\"4_Adversarial_FL.util_functions\")\n",
    "\n",
    "Client = client_module.Client\n",
    "MaliciousClient = malicious_module.MaliciousClient\n",
    "MobileNetV3Transfer = model_module.MobileNetV3Transfer\n",
    "\n",
    "set_seed = utils_module.set_seed\n",
    "evaluate_fn = utils_module.evaluate_fn\n",
    "resolve_callable = utils_module.resolve_callable\n",
    "dist_data_per_client = load_data_module.dist_data_per_client\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using project root: {PROJECT_ROOT}\")\n",
    "print(f\"Device detected: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396eaa3d",
   "metadata": {},
   "source": [
    "## 2. Lab Parameters\n",
    "\n",
    "Tweak these knobs to explore different scenarios. The defaults keep the run lightweight so you can iterate quickly on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6213621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_config = {\n",
    "    \"seed\": 27,\n",
    "    \"data\": {\n",
    "        \"dataset_path\": \"./data\",\n",
    "        \"dataset_name\": \"CIFAR10\",\n",
    "        \"non_iid_per\": 0.2,\n",
    "    },\n",
    "    \"federated\": {\n",
    "        \"num_clients\": 4,\n",
    "        \"fraction_clients\": 0.75,\n",
    "        \"num_rounds\": 2,\n",
    "        \"num_epochs\": 1,\n",
    "        \"batch_size\": 16,\n",
    "        \"local_lr\": 0.05,\n",
    "        \"criterion\": \"torch.nn.CrossEntropyLoss\",\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"num_classes\": 10,\n",
    "        \"pretrained\": False,\n",
    "    },\n",
    "    \"malicious\": {\n",
    "        \"fraction\": 0.5,\n",
    "        \"seed\": 2024,\n",
    "        \"attack\": {\n",
    "            \"type\": \"pgd\",\n",
    "            \"poison_rate\": 0.4,\n",
    "            \"target_label\": 0,\n",
    "            \"epsilon\": 0.03,\n",
    "            \"step_size\": 0.007,\n",
    "            \"iters\": 5,\n",
    "            \"criterion\": \"torch.nn.CrossEntropyLoss\",\n",
    "        },\n",
    "        \"surrogate\": {\n",
    "            \"pretrained\": False,\n",
    "            \"lr\": 5e-4,\n",
    "            \"finetune_epochs\": 1,\n",
    "            \"batch_size\": 16,\n",
    "            \"num_classes\": 10,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "lab_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6373132f",
   "metadata": {},
   "source": [
    "Adjust parameters above as you go. Increasing `num_rounds` or enabling pretrained weights will make runs slower but can highlight longer-term effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7212ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"Current configuration:\")\n",
    "pprint(lab_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adb49b5",
   "metadata": {},
   "source": [
    "## 3. Prepare Client Datasets\n",
    "\n",
    "We reuse Module 3's data pipeline to split CIFAR-10 across clients. The helper caches splits on disk, so subsequent runs are fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36133eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(lab_config[\"seed\"])\n",
    "\n",
    "client_loaders, test_loader = dist_data_per_client(\n",
    "    data_path=lab_config[\"data\"][\"dataset_path\"],\n",
    "    dataset_name=lab_config[\"data\"][\"dataset_name\"],\n",
    "    num_clients=lab_config[\"federated\"][\"num_clients\"],\n",
    "    batch_size=lab_config[\"federated\"][\"batch_size\"],\n",
    "    non_iid_per=lab_config[\"data\"][\"non_iid_per\"],\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "print(f\"Prepared {len(client_loaders)} client loaders\")\n",
    "print(f\"Test loader batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50edad",
   "metadata": {},
   "source": [
    "_Optional:_ peek at a batch to understand the local data. Uncomment the block below to inspect shapes or label counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790cf0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_images, batch_labels = next(iter(client_loaders[0]))\n",
    "# batch_images.shape, batch_labels[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3da9c1",
   "metadata": {},
   "source": [
    "## 4. Build Honest and Malicious Clients\n",
    "\n",
    "Instead of hiding setup logic in a runner, we construct the client pool step by step. This makes it clear how malicious participants differ from honest ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a0de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "criterion_path = lab_config[\"federated\"][\"criterion\"]\n",
    "client_criterion = resolve_callable(criterion_path)()\n",
    "\n",
    "attack_payload = {\n",
    "    key: deepcopy(value)\n",
    "    for key, value in lab_config[\"malicious\"].items()\n",
    "    if key not in {\"fraction\"}\n",
    "}\n",
    "\n",
    "print(\"Attack payload passed to malicious clients:\")\n",
    "pprint(attack_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8872b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(malicious_fraction: float):\n",
    "    '''Instantiate honest and malicious clients for the experiment.'''\n",
    "    malicious_fraction = max(0.0, min(1.0, malicious_fraction))\n",
    "    num_clients = len(client_loaders)\n",
    "    num_malicious = int(np.floor(num_clients * malicious_fraction))\n",
    "\n",
    "    malicious_ids = []\n",
    "    if num_malicious:\n",
    "        malicious_ids = sorted(np.random.choice(num_clients, size=num_malicious, replace=False).tolist())\n",
    "\n",
    "    clients = []\n",
    "    for idx, loader in enumerate(client_loaders):\n",
    "        if idx in malicious_ids:\n",
    "            client = MaliciousClient(\n",
    "                client_id=idx,\n",
    "                local_data=loader,\n",
    "                device=DEVICE,\n",
    "                num_epochs=lab_config[\"federated\"][\"num_epochs\"],\n",
    "                criterion=client_criterion,\n",
    "                lr=lab_config[\"federated\"][\"local_lr\"],\n",
    "                attack_config=deepcopy(attack_payload),\n",
    "            )\n",
    "        else:\n",
    "            client = Client(\n",
    "                client_id=idx,\n",
    "                local_data=loader,\n",
    "                device=DEVICE,\n",
    "                num_epochs=lab_config[\"federated\"][\"num_epochs\"],\n",
    "                criterion=client_criterion,\n",
    "                lr=lab_config[\"federated\"][\"local_lr\"],\n",
    "            )\n",
    "        clients.append(client)\n",
    "\n",
    "    return clients, malicious_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1afcba0",
   "metadata": {},
   "source": [
    "## 5. Federated Training Loop\n",
    "\n",
    "The function below mirrors the FedAvg routine in plain view: sample clients, broadcast the global model, perform local updates, then average the weights. We log which malicious ids participate each round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fedavg(malicious_fraction: float, *, label: str, verbose: bool = True):\n",
    "    set_seed(lab_config[\"seed\"])\n",
    "\n",
    "    clients, malicious_ids = create_clients(malicious_fraction)\n",
    "    num_clients = len(clients)\n",
    "    sample_fraction = lab_config[\"federated\"][\"fraction_clients\"]\n",
    "    num_rounds = lab_config[\"federated\"][\"num_rounds\"]\n",
    "\n",
    "    model_kwargs = {\n",
    "        \"num_classes\": lab_config[\"model\"][\"num_classes\"],\n",
    "        \"pretrained\": lab_config[\"model\"][\"pretrained\"],\n",
    "    }\n",
    "    global_model = MobileNetV3Transfer(**model_kwargs).to(DEVICE)\n",
    "    eval_criterion = resolve_callable(lab_config[\"federated\"][\"criterion\"])()\n",
    "\n",
    "    history = []\n",
    "    metrics = {\"loss\": [], \"accuracy\": []}\n",
    "\n",
    "    for round_idx in range(num_rounds):\n",
    "        num_sampled = max(1, int(np.floor(sample_fraction * num_clients)))\n",
    "        sampled = sorted(np.random.choice(num_clients, size=num_sampled, replace=False).tolist())\n",
    "        active_malicious = [idx for idx in sampled if idx in malicious_ids]\n",
    "        history.append(\n",
    "            {\n",
    "                \"round\": round_idx + 1,\n",
    "                \"sampled\": sampled,\n",
    "                \"malicious\": active_malicious,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for idx in sampled:\n",
    "            client_model = MobileNetV3Transfer(**model_kwargs).to(DEVICE)\n",
    "            client_model.load_state_dict(global_model.state_dict())\n",
    "            clients[idx].x = client_model\n",
    "\n",
    "        for idx in sampled:\n",
    "            clients[idx].client_update()\n",
    "\n",
    "        avg_params = [torch.zeros_like(param, device=DEVICE) for param in global_model.parameters()]\n",
    "        with torch.no_grad():\n",
    "            for idx in sampled:\n",
    "                for avg_param, client_param in zip(avg_params, clients[idx].y.parameters()):\n",
    "                    avg_param.add_(client_param.data / len(sampled))\n",
    "            for param, avg_param in zip(global_model.parameters(), avg_params):\n",
    "                param.data.copy_(avg_param.data)\n",
    "\n",
    "        loss, acc = evaluate_fn(test_loader, global_model, eval_criterion, DEVICE)\n",
    "        metrics[\"loss\"].append(loss)\n",
    "        metrics[\"accuracy\"].append(acc)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"[{label}] Round {round_idx + 1}: sampled={sampled} malicious={active_malicious} \"\n",
    "                f\"loss={loss:.4f} acc={acc:.2f}%\"\n",
    "            )\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"global_model\": global_model,\n",
    "        \"metrics\": metrics,\n",
    "        \"history\": history,\n",
    "        \"malicious_ids\": malicious_ids,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e416b3",
   "metadata": {},
   "source": [
    "## 6. Run Clean vs. Poisoned Experiments\n",
    "\n",
    "Set `RUN_TRAINING` to `True` when you are ready. Start with the defaults, then iterate on the configuration to see how the attack strength shifts the curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ca6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_TRAINING = False\n",
    "clean_run = None\n",
    "attack_run = None\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    clean_run = run_fedavg(0.0, label=\"Clean baseline\")\n",
    "    attack_run = run_fedavg(lab_config[\"malicious\"][\"fraction\"], label=\"Poisoned run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb2a1ee",
   "metadata": {},
   "source": [
    "## 7. Visualise Metrics\n",
    "\n",
    "Overlay the clean and poisoned trajectories to spot divergence. Rerun this cell each time you change the training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed81f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if clean_run and attack_run:\n",
    "    rounds = range(1, len(clean_run[\"metrics\"][\"accuracy\"]) + 1)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(rounds, clean_run[\"metrics\"][\"accuracy\"], marker=\"o\", label=clean_run[\"label\"])\n",
    "    plt.plot(rounds, attack_run[\"metrics\"][\"accuracy\"], marker=\"o\", label=attack_run[\"label\"])\n",
    "    plt.xlabel(\"Communication round\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Clean vs. poisoned global accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(rounds, clean_run[\"metrics\"][\"loss\"], marker=\"o\", label=clean_run[\"label\"])\n",
    "    plt.plot(rounds, attack_run[\"metrics\"][\"loss\"], marker=\"o\", label=attack_run[\"label\"])\n",
    "    plt.xlabel(\"Communication round\")\n",
    "    plt.ylabel(\"Cross-entropy loss\")\n",
    "    plt.title(\"Clean vs. poisoned global loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Train both runs first by setting RUN_TRAINING = True.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e208b73",
   "metadata": {},
   "source": [
    "### Round Participation Log\n",
    "\n",
    "Inspect which clients (and especially which malicious ids) were active each round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e94330",
   "metadata": {},
   "outputs": [],
   "source": [
    "if attack_run:\n",
    "    from pprint import pprint\n",
    "\n",
    "    print(\"Malicious client ids:\", attack_run[\"malicious_ids\"])\n",
    "    print(\"Round activity (poisoned run):\")\n",
    "    pprint(attack_run[\"history\"])\n",
    "else:\n",
    "    print(\"No poisoned run recorded yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203b68a1",
   "metadata": {},
   "source": [
    "## 8. Reflection Prompts\n",
    "\n",
    "- When do the poisoned and clean curves start to diverge? How does that relate to the sampled malicious clients above?\n",
    "- How does changing `poison_rate` or the number of rounds affect the attack's stealthiness?\n",
    "- Try swapping the attack type (e.g. FGSM or random noise) and observe which metrics are most diagnostic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
