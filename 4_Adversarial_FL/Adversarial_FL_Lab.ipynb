{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd92a93f",
   "metadata": {},
   "source": [
    "# Adversarial Federated Learning Lab\n",
    "\n",
    "This guided lab walks through a clean federated learning baseline before escalating into surrogate-driven poisoning attacks. Execute each section in sequence, tweak the configuration knobs, and compare outcomes as you progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169e3d8e",
   "metadata": {},
   "source": [
    "> Tip: duplicate this notebook if you want to keep a personal copy of your experiments or notes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e4777",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "The next cell discovers the repository root, adds it to `sys.path`, and imports the Module 4 utilities you will use throughout the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b380cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "from importlib import import_module\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if not (PROJECT_ROOT / \"4_Adversarial_FL\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "PACKAGE_ROOT = PROJECT_ROOT / \"4_Adversarial_FL\"\n",
    "\n",
    "if not PACKAGE_ROOT.exists():\n",
    "    raise RuntimeError(\"Run this notebook from the repo root or inside 4_Adversarial_FL.\")\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "load_data_module = import_module(\"4_Adversarial_FL.load_data_for_clients\")\n",
    "client_module = import_module(\"4_Adversarial_FL.client\")\n",
    "model_module = import_module(\"4_Adversarial_FL.model\")\n",
    "utils_module = import_module(\"4_Adversarial_FL.util_functions\")\n",
    "attacks_module = import_module(\"4_Adversarial_FL.attacks\")\n",
    "\n",
    "Client = client_module.Client\n",
    "MobileNetV3Transfer = model_module.MobileNetV3Transfer\n",
    "MobileNetV2Transfer = model_module.MobileNetV2Transfer\n",
    "set_seed = utils_module.set_seed\n",
    "evaluate_fn = utils_module.evaluate_fn\n",
    "resolve_callable = utils_module.resolve_callable\n",
    "dist_data_per_client = load_data_module.dist_data_per_client\n",
    "get_attack = attacks_module.get_attack\n",
    "\n",
    "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "IMAGENET_STD = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "\n",
    "def denormalize_batch(batch: torch.Tensor) -> torch.Tensor:\n",
    "    return batch * IMAGENET_STD.to(batch.device) + IMAGENET_MEAN.to(batch.device)\n",
    "\n",
    "\n",
    "def normalize_batch(batch: torch.Tensor) -> torch.Tensor:\n",
    "    return (batch - IMAGENET_MEAN.to(batch.device)) / IMAGENET_STD.to(batch.device)\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using project root: {PROJECT_ROOT}\")\n",
    "print(f\"Device detected: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b76998",
   "metadata": {},
   "source": [
    "## 2. Clean FedAvg Baseline\n",
    "\n",
    "Start with a vanilla FedAvg experiment on CIFAR-10 using a MobileNetV3 backbone. This mirrors the honest training loop from earlier modules and establishes a reference point before any attacks are introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb78ef",
   "metadata": {},
   "source": [
    "### 2.1 Configure baseline hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df4da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_config = {\n",
    "    \"seed\": 27,\n",
    "    \"data\": {\n",
    "        \"dataset_path\": \"./data\",\n",
    "        \"dataset_name\": \"CIFAR10\",\n",
    "        \"non_iid_per\": 0.2,\n",
    "    },\n",
    "    \"federated\": {\n",
    "        \"num_clients\": 4,\n",
    "        \"fraction_clients\": 0.75,\n",
    "        \"num_rounds\": 2,\n",
    "        \"num_epochs\": 1,\n",
    "        \"batch_size\": 16,\n",
    "        \"local_lr\": 0.05,\n",
    "        \"criterion\": \"torch.nn.CrossEntropyLoss\",\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"num_classes\": 10,\n",
    "        \"pretrained\": False,\n",
    "    },\n",
    "}\n",
    "baseline_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e4055",
   "metadata": {},
   "source": [
    "Use a small number of rounds while you are experimenting on CPU. Increase `num_rounds` after you are comfortable with the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b92f5a",
   "metadata": {},
   "source": [
    "### 2.2 Prepare CIFAR-10 client loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ff8984",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(baseline_config[\"seed\"])\n",
    "\n",
    "CLIENT_LOADERS, TEST_LOADER = dist_data_per_client(\n",
    "    data_path=baseline_config[\"data\"][\"dataset_path\"],\n",
    "    dataset_name=baseline_config[\"data\"][\"dataset_name\"],\n",
    "    num_clients=baseline_config[\"federated\"][\"num_clients\"],\n",
    "    batch_size=baseline_config[\"federated\"][\"batch_size\"],\n",
    "    non_iid_per=baseline_config[\"data\"][\"non_iid_per\"],\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "len(CLIENT_LOADERS), len(TEST_LOADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e82801d",
   "metadata": {},
   "source": [
    "_(Optional)_ Inspect a batch to sanity-check shapes and label balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27345a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_images, sample_labels = next(iter(CLIENT_LOADERS[0]))\n",
    "# sample_images.shape, sample_labels[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65f7286",
   "metadata": {},
   "source": [
    "### 2.3 Build honest clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d9ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "criterion_path = baseline_config[\"federated\"][\"criterion\"]\n",
    "honest_criterion = resolve_callable(criterion_path)()\n",
    "\n",
    "\n",
    "def build_honest_clients() -> list[Client]:\n",
    "    clients = []\n",
    "    for idx, loader in enumerate(CLIENT_LOADERS):\n",
    "        clients.append(\n",
    "            Client(\n",
    "                client_id=idx,\n",
    "                local_data=loader,\n",
    "                device=DEVICE,\n",
    "                num_epochs=baseline_config[\"federated\"][\"num_epochs\"],\n",
    "                criterion=honest_criterion,\n",
    "                lr=baseline_config[\"federated\"][\"local_lr\"],\n",
    "            )\n",
    "        )\n",
    "    return clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df63211e",
   "metadata": {},
   "source": [
    "### 2.4 FedAvg training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3158adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def run_fedavg_rounds(clients, config, label: str, *, verbose: bool = True, malicious_ids: list[int] | None = None):\n",
    "    num_clients = len(clients)\n",
    "    num_rounds = config[\"federated\"][\"num_rounds\"]\n",
    "    fraction = config[\"federated\"][\"fraction_clients\"]\n",
    "    model_kwargs = {\n",
    "        \"num_classes\": config[\"model\"][\"num_classes\"],\n",
    "        \"pretrained\": config[\"model\"].get(\"pretrained\", False),\n",
    "    }\n",
    "    eval_criterion = resolve_callable(config[\"federated\"][\"criterion\"])()\n",
    "\n",
    "    malicious_set = set(malicious_ids or [])\n",
    "    history = []\n",
    "    metrics = {\"loss\": [], \"accuracy\": []}\n",
    "    global_model = MobileNetV3Transfer(**model_kwargs).to(DEVICE)\n",
    "\n",
    "    for round_idx in range(num_rounds):\n",
    "        set_seed(config[\"seed\"] + round_idx)\n",
    "        num_sampled = max(1, int(math.ceil(fraction * num_clients)))\n",
    "        sampled = sorted(np.random.choice(num_clients, size=num_sampled, replace=False).tolist())\n",
    "        active_malicious = [idx for idx in sampled if idx in malicious_set]\n",
    "        history.append(\n",
    "            {\n",
    "                \"round\": round_idx + 1,\n",
    "                \"sampled\": sampled,\n",
    "                \"malicious\": active_malicious,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for idx in sampled:\n",
    "            client_model = MobileNetV3Transfer(**model_kwargs).to(DEVICE)\n",
    "            client_model.load_state_dict(global_model.state_dict())\n",
    "            clients[idx].x = client_model\n",
    "\n",
    "        for idx in sampled:\n",
    "            clients[idx].client_update()\n",
    "\n",
    "        avg_params = [torch.zeros_like(param, device=DEVICE) for param in global_model.parameters()]\n",
    "        with torch.no_grad():\n",
    "            for idx in sampled:\n",
    "                for avg_param, client_param in zip(avg_params, clients[idx].y.parameters()):\n",
    "                    avg_param.add_(client_param.data / len(sampled))\n",
    "            for param, avg_param in zip(global_model.parameters(), avg_params):\n",
    "                param.data.copy_(avg_param.data)\n",
    "\n",
    "        loss, acc = evaluate_fn(TEST_LOADER, global_model, eval_criterion, DEVICE)\n",
    "        metrics[\"loss\"].append(loss)\n",
    "        metrics[\"accuracy\"].append(acc)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"[{label}] Round {round_idx + 1}: sampled={sampled} malicious={active_malicious} \"\n",
    "                f\"loss={loss:.4f} acc={acc:.2f}%\"\n",
    "            )\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"global_model\": global_model,\n",
    "        \"metrics\": metrics,\n",
    "        \"history\": history,\n",
    "        \"config\": config,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9957ece1",
   "metadata": {},
   "source": [
    "### 2.5 Execute the clean baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301118f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CLEAN_BASELINE = False\n",
    "baseline_run = None\n",
    "\n",
    "if RUN_CLEAN_BASELINE:\n",
    "    honest_clients = build_honest_clients()\n",
    "    baseline_run = run_fedavg_rounds(honest_clients, baseline_config, label=\"Clean FedAvg\")\n",
    "    print(\"Baseline training complete.\")\n",
    "else:\n",
    "    print(\"Set RUN_CLEAN_BASELINE = True and re-run this cell when you're ready to train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba65c8e",
   "metadata": {},
   "source": [
    "### 2.6 Visualise clean accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f7fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline_run:\n",
    "    rounds = range(1, len(baseline_run[\"metrics\"][\"accuracy\"]) + 1)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(rounds, baseline_run[\"metrics\"][\"accuracy\"], marker=\"o\", label=baseline_run[\"label\"])\n",
    "    plt.xlabel(\"Communication round\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Clean FedAvg test accuracy\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(rounds, baseline_run[\"metrics\"][\"loss\"], marker=\"o\", label=baseline_run[\"label\"])\n",
    "    plt.xlabel(\"Communication round\")\n",
    "    plt.ylabel(\"Cross-entropy loss\")\n",
    "    plt.title(\"Clean FedAvg test loss\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Train the clean baseline first to unlock these plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a80fcb",
   "metadata": {},
   "source": [
    "## 3. Attack Primer\n",
    "\n",
    "Before diving into code, distinguish the main threat families:\n",
    "\n",
    "- **Data poisoning**: adversaries tamper with local training data so their updates steer the global model toward bad behaviour.\n",
    "- **Model poisoning**: adversaries craft updates directly in parameter space (e.g., model replacement).\n",
    "- **Evasion attacks**: adversaries craft examples at inference time without altering the training process.\n",
    "\n",
    "This lab focuses on *surrogate-driven data poisoning* where malicious clients inject adversarial examples into their minibatches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afb069a",
   "metadata": {},
   "source": [
    "## 4. White-box vs. Black-box Surrogates\n",
    "\n",
    "| Setting | Attacker knowledge | Typical strategy |\n",
    "| --- | --- | --- |\n",
    "| White-box | Full access to the victim architecture and weights | Craft gradients directly against the deployed model. |\n",
    "| Black-box | Only inputs/outputs are observable | Train a *surrogate* model that approximates the victim, then transfer attacks crafted on the surrogate. |\n",
    "\n",
    "Federated learning often falls between these extremes—clients know the architecture but only see periodic model snapshots. The surrogate approach you will implement assumes the attacker can approximate the victim with a related backbone (MobileNetV2 vs. MobileNetV3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b978e58c",
   "metadata": {},
   "source": [
    "## 5. Train a Surrogate Model (MobileNetV2)\n",
    "\n",
    "Pick one client's data to mimic the attacker's perspective and fine-tune a MobileNetV2 surrogate. The surrogate does *not* match the server's MobileNetV3 exactly, which reflects the partial knowledge black-box scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f068fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_config = {\n",
    "    \"client_id\": 0,\n",
    "    \"epochs\": 1,\n",
    "    \"lr\": 5e-4,\n",
    "    \"batch_size\": baseline_config[\"federated\"][\"batch_size\"],\n",
    "    \"criterion\": \"torch.nn.CrossEntropyLoss\",\n",
    "}\n",
    "\n",
    "surrogate_dataset = CLIENT_LOADERS[surrogate_config[\"client_id\"]].dataset\n",
    "surrogate_loader = DataLoader(\n",
    "    surrogate_dataset,\n",
    "    batch_size=surrogate_config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "surrogate_model = MobileNetV2Transfer(\n",
    "    pretrained=False,\n",
    "    num_classes=baseline_config[\"model\"][\"num_classes\"],\n",
    ").to(DEVICE)\n",
    "\n",
    "surrogate_optimizer = torch.optim.Adam(surrogate_model.parameters(), lr=surrogate_config[\"lr\"])\n",
    "surrogate_criterion = resolve_callable(surrogate_config[\"criterion\"])()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93925a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_SURROGATE_TRAINING = False\n",
    "\n",
    "if RUN_SURROGATE_TRAINING:\n",
    "    set_seed(baseline_config[\"seed\"])\n",
    "    surrogate_model.train()\n",
    "    for epoch in range(surrogate_config[\"epochs\"]):\n",
    "        epoch_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for images, labels in surrogate_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            surrogate_optimizer.zero_grad()\n",
    "            logits = surrogate_model(images)\n",
    "            loss = surrogate_criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            surrogate_optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * labels.size(0)\n",
    "            total += labels.size(0)\n",
    "            correct += (logits.argmax(dim=1) == labels).sum().item()\n",
    "        avg_loss = epoch_loss / max(total, 1)\n",
    "        acc = 100 * correct / max(total, 1)\n",
    "        print(f\"Epoch {epoch + 1}: loss={avg_loss:.4f} acc={acc:.2f}%\")\n",
    "else:\n",
    "    print(\"Set RUN_SURROGATE_TRAINING = True to fine-tune the surrogate model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1c2eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(loader, model, description: str):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            preds = model(images).argmax(dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    acc = 100 * correct / max(total, 1)\n",
    "    print(f\"{description} accuracy: {acc:.2f}% ({correct}/{total})\")\n",
    "\n",
    "\n",
    "evaluate_model(surrogate_loader, surrogate_model, \"Surrogate (local shard)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb596846",
   "metadata": {},
   "source": [
    "## 6. Craft Attacks on the Surrogate\n",
    "\n",
    "Choose an attack family and hyperparameters, then generate adversarial examples with the trained surrogate. The crafted samples will later be replayed inside the federated training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6357d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_recipe = {\n",
    "    \"type\": \"pgd\",           # options: \"pgd\", \"fgsm\", \"rand_noise\"\n",
    "    \"poison_rate\": 0.4,       # fraction of a malicious minibatch to replace\n",
    "    \"target_label\": 0,        # targeted misclassification (set to None for untargeted)\n",
    "    \"epsilon\": 0.03,          # PGD budget\n",
    "    \"step_size\": 0.007,       # step size for PGD/FGSM/random noise\n",
    "    \"iters\": 5,               # PGD iterations\n",
    "    \"criterion\": \"torch.nn.CrossEntropyLoss\",\n",
    "}\n",
    "\n",
    "attack_fn = get_attack(attack_recipe[\"type\"])\n",
    "attack_criterion = resolve_callable(attack_recipe[\"criterion\"])()\n",
    "\n",
    "print(\"Configured attack:\")\n",
    "attack_recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28281a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_attack_labels(labels: torch.Tensor, recipe: dict) -> torch.Tensor:\n",
    "    target_label = recipe.get(\"target_label\")\n",
    "    if target_label is None:\n",
    "        return labels\n",
    "    return torch.full_like(labels, int(target_label))\n",
    "\n",
    "\n",
    "def craft_adversarial_examples(images: torch.Tensor, labels: torch.Tensor, *, surrogate, attack_fn, attack_criterion, recipe: dict) -> torch.Tensor:\n",
    "    surrogate.eval()\n",
    "    images = images.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "\n",
    "    attack_name = recipe[\"type\"].lower()\n",
    "    denorm = denormalize_batch(images)\n",
    "\n",
    "    if attack_name == \"pgd\":\n",
    "        adv_denorm = attack_fn(\n",
    "            model=surrogate,\n",
    "            criterion=attack_criterion,\n",
    "            images=denorm,\n",
    "            labels=labels,\n",
    "            eps=recipe.get(\"epsilon\", 0.03),\n",
    "            step_size=recipe.get(\"step_size\", 0.007),\n",
    "            iters=recipe.get(\"iters\", 5),\n",
    "        )\n",
    "    elif attack_name == \"fgsm\":\n",
    "        adv_denorm = attack_fn(\n",
    "            model=surrogate,\n",
    "            criterion=attack_criterion,\n",
    "            images=denorm,\n",
    "            labels=labels,\n",
    "            step_size=recipe.get(\"step_size\", 0.003),\n",
    "        )\n",
    "    else:\n",
    "        adv_denorm = attack_fn(\n",
    "            denorm,\n",
    "            step_size=recipe.get(\"step_size\", 0.003),\n",
    "        )\n",
    "\n",
    "    return normalize_batch(adv_denorm)\n",
    "\n",
    "\n",
    "def plot_clean_vs_adv(clean_batch: torch.Tensor, adv_batch: torch.Tensor, index: int = 0) -> None:\n",
    "    clean_img = denormalize_batch(clean_batch[index:index + 1]).squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    adv_img = denormalize_batch(adv_batch[index:index + 1]).squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    axes[0].imshow(np.clip(clean_img, 0, 1))\n",
    "    axes[0].set_title(\"Clean\")\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[1].imshow(np.clip(adv_img, 0, 1))\n",
    "    axes[1].set_title(\"Adversarial\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8543a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = next(iter(surrogate_loader))\n",
    "clean_images, clean_labels = sample_batch\n",
    "attack_labels = prepare_attack_labels(clean_labels, attack_recipe)\n",
    "adversarial_images = craft_adversarial_examples(\n",
    "    clean_images,\n",
    "    attack_labels,\n",
    "    surrogate=surrogate_model,\n",
    "    attack_fn=attack_fn,\n",
    "    attack_criterion=attack_criterion,\n",
    "    recipe=attack_recipe,\n",
    ")\n",
    "\n",
    "plot_clean_vs_adv(clean_images, adversarial_images, index=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1208b2f8",
   "metadata": {},
   "source": [
    "## 7. Deploy the Attack Against the Clean Model\n",
    "\n",
    "With adversarial samples ready, create malicious clients that replay the crafted batches during local training. These clients reuse the surrogate (MobileNetV2) to generate poison on the fly and push the global MobileNetV3 away from its clean trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d046425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurrogateAttackClient(Client):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        client_id: int,\n",
    "        local_data,\n",
    "        device: torch.device,\n",
    "        num_epochs: int,\n",
    "        criterion,\n",
    "        lr: float,\n",
    "        surrogate_state,\n",
    "        attack_fn,\n",
    "        attack_criterion,\n",
    "        recipe: dict,\n",
    "        num_classes: int,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            client_id=client_id,\n",
    "            local_data=local_data,\n",
    "            device=device,\n",
    "            num_epochs=num_epochs,\n",
    "            criterion=criterion,\n",
    "            lr=lr,\n",
    "        )\n",
    "        self.recipe = recipe\n",
    "        self.attack_fn = attack_fn\n",
    "        self.attack_criterion = attack_criterion\n",
    "        self.poison_rate = float(recipe.get(\"poison_rate\", 0.0))\n",
    "        self.target_label = recipe.get(\"target_label\")\n",
    "        self.surrogate = MobileNetV2Transfer(pretrained=False, num_classes=num_classes).to(self.device)\n",
    "        self.surrogate.load_state_dict(surrogate_state)\n",
    "        self.surrogate.eval()\n",
    "\n",
    "    def client_update(self) -> None:\n",
    "        if self.x is None:\n",
    "            raise ValueError(\"Client model `x` has not been initialised by the server.\")\n",
    "\n",
    "        self.y = deepcopy(self.x).to(self.device)\n",
    "        self.y.train()\n",
    "\n",
    "        for _ in range(self.num_epochs):\n",
    "            for inputs, labels in self.data:\n",
    "                inputs = inputs.float().to(self.device)\n",
    "                labels = labels.long().to(self.device)\n",
    "\n",
    "                if self.poison_rate > 0.0:\n",
    "                    mask = torch.rand(labels.size(0), device=self.device) < self.poison_rate\n",
    "                    if mask.any():\n",
    "                        target_labels = labels[mask]\n",
    "                        if self.target_label is not None:\n",
    "                            target_labels = torch.full_like(target_labels, int(self.target_label))\n",
    "                        poisoned = craft_adversarial_examples(\n",
    "                            inputs[mask],\n",
    "                            target_labels,\n",
    "                            surrogate=self.surrogate,\n",
    "                            attack_fn=self.attack_fn,\n",
    "                            attack_criterion=self.attack_criterion,\n",
    "                            recipe=self.recipe,\n",
    "                        )\n",
    "                        inputs = inputs.clone()\n",
    "                        labels = labels.clone()\n",
    "                        inputs[mask] = poisoned\n",
    "                        labels[mask] = target_labels\n",
    "\n",
    "                outputs = self.y(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                grads = torch.autograd.grad(loss, self.y.parameters())\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for param, grad in zip(self.y.parameters(), grads):\n",
    "                        param.data -= self.lr * grad.data\n",
    "\n",
    "            if self.device.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a45c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_attack_clients(recipe: dict, malicious_fraction: float, *, seed: int, surrogate_state) -> tuple[list[Client], list[int]]:\n",
    "    num_clients = len(CLIENT_LOADERS)\n",
    "    malicious_fraction = max(0.0, min(1.0, malicious_fraction))\n",
    "    num_malicious = int(np.floor(num_clients * malicious_fraction))\n",
    "    rng = np.random.default_rng(seed)\n",
    "    malicious_ids = []\n",
    "    if num_malicious > 0:\n",
    "        malicious_ids = sorted(rng.choice(num_clients, size=num_malicious, replace=False).tolist())\n",
    "\n",
    "    clients: list[Client] = []\n",
    "    attack_fn_local = get_attack(recipe[\"type\"])\n",
    "    attack_criterion_local = resolve_callable(recipe.get(\"criterion\", \"torch.nn.CrossEntropyLoss\"))()\n",
    "    for idx, loader in enumerate(CLIENT_LOADERS):\n",
    "        if idx in malicious_ids:\n",
    "            client = SurrogateAttackClient(\n",
    "                client_id=idx,\n",
    "                local_data=loader,\n",
    "                device=DEVICE,\n",
    "                num_epochs=baseline_config[\"federated\"][\"num_epochs\"],\n",
    "                criterion=honest_criterion,\n",
    "                lr=baseline_config[\"federated\"][\"local_lr\"],\n",
    "                surrogate_state=surrogate_state,\n",
    "                attack_fn=attack_fn_local,\n",
    "                attack_criterion=attack_criterion_local,\n",
    "                recipe=recipe,\n",
    "                num_classes=baseline_config[\"model\"][\"num_classes\"],\n",
    "            )\n",
    "        else:\n",
    "            client = Client(\n",
    "                client_id=idx,\n",
    "                local_data=loader,\n",
    "                device=DEVICE,\n",
    "                num_epochs=baseline_config[\"federated\"][\"num_epochs\"],\n",
    "                criterion=honest_criterion,\n",
    "                lr=baseline_config[\"federated\"][\"local_lr\"],\n",
    "            )\n",
    "        clients.append(client)\n",
    "\n",
    "    return clients, malicious_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcf6cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_state = surrogate_model.state_dict()\n",
    "attack_run_config = {\n",
    "    \"malicious_fraction\": 0.5,\n",
    "    \"seed\": 2024,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a365c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_POISONED_TRAINING = False\n",
    "poisoned_run = None\n",
    "malicious_ids = []\n",
    "\n",
    "if RUN_POISONED_TRAINING:\n",
    "    attack_clients, malicious_ids = build_attack_clients(\n",
    "        attack_recipe,\n",
    "        attack_run_config[\"malicious_fraction\"],\n",
    "        seed=attack_run_config[\"seed\"],\n",
    "        surrogate_state=surrogate_state,\n",
    "    )\n",
    "    poisoned_run = run_fedavg_rounds(\n",
    "        attack_clients,\n",
    "        baseline_config,\n",
    "        label=\"Surrogate attack\",\n",
    "        malicious_ids=malicious_ids,\n",
    "    )\n",
    "    print(\"Poisoned training complete.\")\n",
    "else:\n",
    "    print(\"Enable RUN_POISONED_TRAINING to launch the adversarial run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9409649d",
   "metadata": {},
   "source": [
    "### 7.1 Compare clean and poisoned trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline_run and poisoned_run:\n",
    "    rounds = range(1, len(poisoned_run[\"metrics\"][\"accuracy\"]) + 1)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(rounds, baseline_run[\"metrics\"][\"accuracy\"], marker=\"o\", label=baseline_run[\"label\"])\n",
    "    plt.plot(rounds, poisoned_run[\"metrics\"][\"accuracy\"], marker=\"o\", label=poisoned_run[\"label\"])\n",
    "    plt.xlabel(\"Communication round\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Clean vs. poisoned global accuracy\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(rounds, baseline_run[\"metrics\"][\"loss\"], marker=\"o\", label=baseline_run[\"label\"])\n",
    "    plt.plot(rounds, poisoned_run[\"metrics\"][\"loss\"], marker=\"o\", label=poisoned_run[\"label\"])\n",
    "    plt.xlabel(\"Communication round\")\n",
    "    plt.ylabel(\"Cross-entropy loss\")\n",
    "    plt.title(\"Clean vs. poisoned global loss\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Train both the clean and poisoned runs to compare metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3e295b",
   "metadata": {},
   "source": [
    "### 7.2 Inspect malicious participation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "if poisoned_run:\n",
    "    print(\"Malicious client ids:\", malicious_ids)\n",
    "    print(\"Round-by-round participation:\")\n",
    "    for record in poisoned_run[\"history\"]:\n",
    "        print(record)\n",
    "else:\n",
    "    print(\"No poisoned run to summarise yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762abcd9",
   "metadata": {},
   "source": [
    "## 8. Reflection Prompts\n",
    "\n",
    "- How quickly do the poisoned metrics diverge from the clean baseline?\n",
    "- What happens if you vary the malicious fraction or poison rate?\n",
    "- Try alternative attacks (`fgsm`, `rand_noise`) and compare their transferability to the victim model.\n",
    "- Swap the surrogate backbone or training schedule—how sensitive is the attack to architectural mismatches?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
