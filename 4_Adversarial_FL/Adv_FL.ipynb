{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f797f8be",
   "metadata": {},
   "source": [
    "# Module 4 Adversarial FL \u2013 Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bd5786",
   "metadata": {},
   "source": [
    "## 1. Federated Baseline Imports\n",
    "\n",
    "Import core utilities (config loader, model helpers, FL runners) plus PyTorch, NumPy, and logging so the notebook can spin up federated experiments reproducibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df7305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as tv_transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "\n",
    "from util_functions import set_seed, evaluate_fn, run_fl\n",
    "from load_data_for_clients import dist_data_per_client\n",
    "from algos import (\n",
    "    Server,\n",
    "    ScaffoldServer,\n",
    "    FedAdamServer,\n",
    "    FedAdagradServer,\n",
    "    FedYogiServer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c18032",
   "metadata": {},
   "source": [
    "## 2. Federated Baseline Paths & Config\n",
    "\n",
    "Load `config.yaml`, seed all RNGs, and capture the global/data/model sections that drive every experiment. We also build the algorithm map so we can instantiate any server class by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370afa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = Path(\"config.yaml\")\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(\"Could not locate config.yaml in the working directory\")\n",
    "with CONFIG_PATH.open() as f:\n",
    "    CONFIG = yaml.safe_load(f)\n",
    "global_config = CONFIG.get(\"global_config\", {})\n",
    "data_config = CONFIG.get(\"data_config\", {})\n",
    "model_config = CONFIG.get(\"model_config\", {})\n",
    "alg_configs = CONFIG.get(\"algorithms\", {})\n",
    "attack_defaults = CONFIG.get(\"attack\", {})\n",
    "set_seed(global_config.get(\"seed\", 42))\n",
    "\n",
    "\n",
    "def get_device(preferred: str | None = None) -> torch.device:\n",
    "    choice = preferred if preferred is not None else global_config.get(\"device\")\n",
    "    if isinstance(choice, str):\n",
    "        if choice.startswith(\"cuda\") and not torch.cuda.is_available():\n",
    "            choice = \"cpu\"\n",
    "        return torch.device(choice)\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DEVICE = get_device()\n",
    "AVAILABLE_ALGORITHMS = list(alg_configs)\n",
    "print(\"Loaded config from\", CONFIG_PATH.resolve())\n",
    "print(\"Available algorithms:\", AVAILABLE_ALGORITHMS)\n",
    "\n",
    "ARTIFACT_DIR = Path('artifacts')\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd69c4dd",
   "metadata": {},
   "source": [
    "## 3. Federated Baseline Helpers\n",
    "\n",
    "Utility wrappers for running a single algorithm, looping over all configured algorithms, and summarising server outputs (loss/accuracy plus training history). These ensure every later experiment shares the same execution pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHM_MAP = {\n",
    "    \"FedAvg\": Server,\n",
    "    \"Scaffold\": ScaffoldServer,\n",
    "    \"FedAdam\": FedAdamServer,\n",
    "    \"FedAdagrad\": FedAdagradServer,\n",
    "    \"FedYogi\": FedYogiServer,\n",
    "}\n",
    "\n",
    "missing = sorted(set(AVAILABLE_ALGORITHMS) - set(ALGORITHM_MAP))\n",
    "if missing:\n",
    "    raise KeyError(f\"No server mapping registered for: {missing}\")\n",
    "\n",
    "\n",
    "def train_server(alg_name: str, attack_cfg: dict | None = None):\n",
    "    if alg_name not in alg_configs:\n",
    "        raise ValueError(f\"Algorithm {alg_name!r} not found in configuration.\")\n",
    "\n",
    "    alg_conf = alg_configs[alg_name]\n",
    "    fed_cfg = deepcopy(alg_conf[\"fed_config\"])\n",
    "    fed_cfg[\"algorithm\"] = alg_name\n",
    "    optim_cfg = deepcopy(alg_conf.get(\"optim_config\", {}))\n",
    "    attack_cfg = deepcopy(attack_cfg or {\"malicious_fraction\": 0.0})\n",
    "\n",
    "    return run_fl(\n",
    "        ALGORITHM_MAP[alg_name],\n",
    "        global_config,\n",
    "        data_config,\n",
    "        fed_cfg,\n",
    "        model_config,\n",
    "        optim_cfg,\n",
    "        attack_cfg,\n",
    "    )\n",
    "\n",
    "\n",
    "def summarise_server(server) -> dict:\n",
    "    loss, acc = evaluate_fn(server.data, server.x, server.criterion, server.device)\n",
    "    history = server.results if hasattr(server, \"results\") else {}\n",
    "    return {\n",
    "        \"final_loss\": float(loss),\n",
    "        \"final_accuracy\": float(acc),\n",
    "        \"history\": {\n",
    "            \"loss\": list(history.get(\"loss\", [])),\n",
    "            \"accuracy\": list(history.get(\"accuracy\", [])),\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def run_one_algorithm(alg_name: str, attack_cfg: dict | None = None) -> dict:\n",
    "    server = train_server(alg_name, attack_cfg=attack_cfg)\n",
    "    summary = summarise_server(server)\n",
    "    del server\n",
    "    torch.cuda.empty_cache()\n",
    "    return summary\n",
    "\n",
    "\n",
    "def run_all_algorithms(\n",
    "    algorithms: list[str] | None = None,\n",
    "    attack_cfg: dict | None = None,\n",
    ") -> dict:\n",
    "    algorithms = algorithms or AVAILABLE_ALGORITHMS\n",
    "    results: dict[str, dict] = {}\n",
    "    for name in algorithms:\n",
    "        results[name] = run_one_algorithm(name, attack_cfg=attack_cfg)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce1878b",
   "metadata": {},
   "source": [
    "## 4. Federated Baseline Runs\n",
    "\n",
    "Kick off the clean (non-adversarial) benchmark: run FedAvg once to check the pipeline, then iterate across all algorithms to produce a dictionary of baseline metrics we can revisit later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2929a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_ALGORITHMS = [\"FedAvg\"]  \n",
    "\n",
    "baseline_results = run_all_algorithms(BASELINE_ALGORITHMS)\n",
    "baseline_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist federated baselines\n",
    "\n",
    "Store the clean FedAvg baseline so you can compare future attack runs without rerunning the entire notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "baseline_path = ARTIFACT_DIR / 'module4_federated_baseline.json'\n",
    "with baseline_path.open('w') as f:\n",
    "    json.dump(baseline_results, f, indent=2)\n",
    "print(f'Saved baseline metrics to {baseline_path.resolve()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline validation\n",
    "\n",
    "Check that every baseline algorithm logged the expected number of communication rounds.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def validate_baseline(results, algorithm_config):\n",
    "    issues = []\n",
    "    for name, summary in results.items():\n",
    "        expected = algorithm_config[name]['fed_config']['num_rounds']\n",
    "        actual = len(summary.get('history', {}).get('accuracy', []))\n",
    "        if actual < expected:\n",
    "            issues.append(f\"{name}: expected {expected} rounds, saw {actual}\")\n",
    "    if issues:\n",
    "        raise ValueError('Baseline validation failed:\n",
    "' + '\n",
    "'.join(issues))\n",
    "    print('Baseline validation passed for', ', '.join(sorted(results)))\n",
    "\n",
    "validate_baseline(baseline_results, alg_configs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ed3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_baseline_results(results: dict[str, dict]) -> None:\n",
    "    if not results:\n",
    "        print(\"Baseline run has not been executed yet.\")\n",
    "        return\n",
    "    algs = sorted(results.keys())\n",
    "    accuracies = []\n",
    "    for alg in algs:\n",
    "        summary = results.get(alg, {})\n",
    "        acc = summary.get(\"final_accuracy\")\n",
    "        accuracies.append(float(acc) if acc is not None else 0.0)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(algs, accuracies, color=\"#4c72b0\", alpha=0.85)\n",
    "    plt.ylabel('Final accuracy (%)')\n",
    "    plt.xlabel('Algorithm')\n",
    "    plt.title('Baseline federated accuracy')\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_baseline_results(baseline_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b5585",
   "metadata": {},
   "source": [
    "## 5. Surrogate Imports\n",
    "\n",
    "Add the extra build blocks needed for the attacker: surrogate helpers, malicious client hooks, registry lookups, and MobileNet backbones for black-box transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e69fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from malicious_client import MaliciousClient\n",
    "from model import MobileNetV2Transfer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40898ff3",
   "metadata": {},
   "source": [
    "## 6. Surrogate Paths & Config\n",
    "\n",
    "Construct deterministic data loaders for the surrogate. We pool multiple client shards, optionally apply simple augmentations, and cache the test loader so both notebook and malicious clients share the same view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ffef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SURROGATE_CFG = CONFIG.get(\"surrogate\", {})\n",
    "SURROGATE_CLIENT_ID = int(SURROGATE_CFG.get(\"client_id\", 0))\n",
    "SURROGATE_SEED = SURROGATE_CFG.get(\"seed\", global_config.get(\"seed\", 42))\n",
    "SURROGATE_POOL_SIZE = max(1, int(SURROGATE_CFG.get(\"pool_size\", 1)))\n",
    "\n",
    "_SURROGATE_CLIENT_LOADERS = None\n",
    "_SURROGATE_TEST_LOADER = None\n",
    "_SURROGATE_POOLED_LOADER = None\n",
    "\n",
    "\n",
    "class AugmentedDataset(Dataset):\n",
    "    def __init__(self, base_dataset, transform=None):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.base_dataset[index]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def _surrogate_transform():\n",
    "    if not SURROGATE_CFG.get(\"augment\", False):\n",
    "        return None\n",
    "    return tv_transforms.Compose([\n",
    "        tv_transforms.RandomHorizontalFlip(),\n",
    "        tv_transforms.RandomRotation(degrees=10),\n",
    "    ])\n",
    "\n",
    "\n",
    "def _ensure_surrogate_data():\n",
    "    global _SURROGATE_CLIENT_LOADERS, _SURROGATE_TEST_LOADER\n",
    "    if _SURROGATE_CLIENT_LOADERS is not None and _SURROGATE_TEST_LOADER is not None:\n",
    "        return _SURROGATE_CLIENT_LOADERS, _SURROGATE_TEST_LOADER\n",
    "\n",
    "    loaders, test_loader = dist_data_per_client(\n",
    "        data_path=SURROGATE_CFG.get(\"dataset_path\", data_config.get(\"dataset_path\")),\n",
    "        dataset_name=SURROGATE_CFG.get(\"dataset_name\", data_config.get(\"dataset_name\")),\n",
    "        num_clients=SURROGATE_CFG.get(\"num_clients\", data_config.get(\"num_clients\", 50)),\n",
    "        batch_size=SURROGATE_CFG.get(\"batch_size\", data_config.get(\"batch_size\", 96)),\n",
    "        non_iid_per=SURROGATE_CFG.get(\"non_iid_per\", data_config.get(\"non_iid_per\", 0.0)),\n",
    "        device=get_device(),\n",
    "    )\n",
    "    _SURROGATE_CLIENT_LOADERS = loaders\n",
    "    _SURROGATE_TEST_LOADER = test_loader\n",
    "    return _SURROGATE_CLIENT_LOADERS, _SURROGATE_TEST_LOADER\n",
    "\n",
    "\n",
    "def _build_surrogate_pool_loader():\n",
    "    global _SURROGATE_POOLED_LOADER\n",
    "    if _SURROGATE_POOLED_LOADER is not None:\n",
    "        return _SURROGATE_POOLED_LOADER\n",
    "\n",
    "    client_loaders, _ = _ensure_surrogate_data()\n",
    "    pool_size = min(SURROGATE_POOL_SIZE, len(client_loaders))\n",
    "    datasets = []\n",
    "    for idx in range(pool_size):\n",
    "        dataset = getattr(client_loaders[idx], \"dataset\", None)\n",
    "        if dataset is None:\n",
    "            raise ValueError(\"Client loader is missing a dataset attribute; cannot build surrogate pool\")\n",
    "        datasets.append(dataset)\n",
    "\n",
    "    pooled_dataset = ConcatDataset(datasets)\n",
    "    transform = _surrogate_transform()\n",
    "    if transform is not None:\n",
    "        pooled_dataset = AugmentedDataset(pooled_dataset, transform=transform)\n",
    "\n",
    "    batch_size = int(SURROGATE_CFG.get(\"batch_size\", data_config.get(\"batch_size\", 96)))\n",
    "    _SURROGATE_POOLED_LOADER = DataLoader(\n",
    "        pooled_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    return _SURROGATE_POOLED_LOADER\n",
    "\n",
    "\n",
    "def get_surrogate_train_loader(pool: bool = True):\n",
    "    client_loaders, _ = _ensure_surrogate_data()\n",
    "    if not pool:\n",
    "        if SURROGATE_CLIENT_ID >= len(client_loaders):\n",
    "            raise IndexError(\"Surrogate client id {SURROGATE_CLIENT_ID} out of range for {len(client_loaders)} clients\")\n",
    "        return client_loaders[SURROGATE_CLIENT_ID]\n",
    "    return _build_surrogate_pool_loader()\n",
    "\n",
    "\n",
    "def get_surrogate_test_loader():\n",
    "    _, test_loader = _ensure_surrogate_data()\n",
    "    return test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b5c8d9",
   "metadata": {},
   "source": [
    "## 7. Surrogate Baseline\n",
    "\n",
    "Fine-tune the surrogate MobileNetV2 against the pooled client data. We support freezing the backbone, early stopping, and weight decay so the surrogate\u2019s test accuracy stays in the same ballpark as the federated baseline before attacks begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf56f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_surrogate_model(num_classes: int = 10, pretrained: bool | None = None) -> torch.nn.Module:\n",
    "    if pretrained is None:\n",
    "        pretrained = SURROGATE_CFG.get(\"pretrained\", True)\n",
    "    return MobileNetV2Transfer(pretrained=pretrained, num_classes=num_classes)\n",
    "def train_surrogate_baseline(num_epochs: int | None = None):\n",
    "    set_seed(global_config.get(\"seed\", 42))\n",
    "    train_loader = get_surrogate_train_loader(pool=True)\n",
    "    model = build_surrogate_model(num_classes=SURROGATE_CFG.get(\"num_classes\", model_config.get(\"kwargs\", {}).get(\"num_classes\", 10))).to(get_device())\n",
    "    if SURROGATE_CFG.get(\"freeze_backbone\", False) and hasattr(model, \"v2model\"):\n",
    "        for param in model.v2model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(get_device())\n",
    "    lr = SURROGATE_CFG.get(\"learning_rate\", SURROGATE_CFG.get(\"lr\", 1e-3))\n",
    "    weight_decay = SURROGATE_CFG.get(\"weight_decay\", 0.0)\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    if not trainable_params:\n",
    "        raise RuntimeError(\"No trainable parameters available for surrogate optimisation.\")\n",
    "    optimizer = torch.optim.Adam(\n",
    "        trainable_params,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "    epochs = num_epochs or SURROGATE_CFG.get(\"num_epochs\", 5)\n",
    "    patience = int(SURROGATE_CFG.get(\"early_stop_patience\", 0))\n",
    "    history = {\"loss\": [], \"accuracy\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
    "    best_state = None\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_since_improved = 0\n",
    "    test_loader = get_surrogate_test_loader()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(get_device()).float()\n",
    "            labels = labels.to(get_device()).long()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "        epoch_loss = running_loss / max(len(train_loader), 1)\n",
    "        epoch_acc = 100.0 * correct / max(total, 1)\n",
    "        val_loss, val_acc = evaluate_fn(test_loader, model, criterion, get_device())\n",
    "        history[\"loss\"].append(epoch_loss)\n",
    "        history[\"accuracy\"].append(epoch_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_accuracy\"].append(val_acc)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}: train_loss={epoch_loss:.4f}, train_acc={epoch_acc:.2f}%, val_loss={val_loss:.4f}, val_acc={val_acc:.2f}%\")\n",
    "        if val_loss + 1e-5 < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = deepcopy(model.state_dict())\n",
    "            epochs_since_improved = 0\n",
    "        else:\n",
    "            epochs_since_improved += 1\n",
    "            if patience and epochs_since_improved >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch + 1} after {patience} epochs without improvement.\")\n",
    "                break\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    test_loss, test_acc = evaluate_fn(test_loader, model, criterion, get_device())\n",
    "    summary = {\n",
    "        \"history\": history,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_accuracy\": test_acc,\n",
    "    }\n",
    "    return model, summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa26462",
   "metadata": {},
   "source": [
    "## 8. Baseline Comparison\n",
    "\n",
    "Display the federated-reference metrics alongside the surrogate\u2019s evaluation to confirm the two models are comparable prior to introducing malicious behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a3c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_model, surrogate_summary = train_surrogate_baseline()\n",
    "\n",
    "fedavg_summary = baseline_results.get(\"FedAvg\", {})\n",
    "print(\"Federated baseline:\", fedavg_summary)\n",
    "print(\n",
    "    f\"Surrogate test metrics \u2192 loss: {surrogate_summary['test_loss']:.4f}, accuracy: {surrogate_summary['test_accuracy']:.2f}%\"\n",
    ")\n",
    "\n",
    "surrogate_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surrogate sanity check\n",
    "\n",
    "Save the surrogate training history and ensure the held-out accuracy is reasonable before crafting attacks.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "surrogate_path = ARTIFACT_DIR / 'module4_surrogate.json'\n",
    "with surrogate_path.open('w') as f:\n",
    "    json.dump(surrogate_summary, f, indent=2)\n",
    "acc = surrogate_summary.get('test_accuracy', 0.0)\n",
    "if acc < 5.0:\n",
    "    raise ValueError(f'Surrogate accuracy {acc:.2f}% is suspiciously low; revisit training settings.')\n",
    "print(f'Surrogate test accuracy: {acc:.2f}% (saved details to {surrogate_path.resolve()})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b03c728",
   "metadata": {},
   "source": [
    "### Surrogate Training Curves\n",
    "\n",
    "Track training vs. validation performance of the surrogate to confirm regularisation keeps it aligned with the federated baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4394321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_surrogate_history(summary: dict) -> None:\n",
    "    history = summary.get(\"history\", {})\n",
    "    epochs = range(1, len(history.get(\"loss\", [])) + 1)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.get(\"loss\", []), label=\"train\")\n",
    "    plt.plot(epochs, history.get(\"val_loss\", []), label=\"val\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Surrogate loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.get(\"accuracy\", []), label=\"train\")\n",
    "    plt.plot(epochs, history.get(\"val_accuracy\", []), label=\"val\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Surrogate accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_surrogate_history(surrogate_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125ba958",
   "metadata": {},
   "source": [
    "## 10. Attack Paths & Config\n",
    "\n",
    "Extract the attack defaults (malicious fraction, PGD/FGSM parameters, poison schedule) and build lightweight config builders that let each recipe override pieces of that base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d003e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK_RAW = attack_defaults\n",
    "ATTACK_BASE = ATTACK_RAW.get(\"attack\", {})\n",
    "SURROGATE_BASE = ATTACK_RAW.get(\"surrogate\", {})\n",
    "ATTACK_SEED = ATTACK_RAW.get(\"seed\", global_config.get(\"seed\", 42))\n",
    "BASE_MALICIOUS_FRACTION = ATTACK_RAW.get(\"malicious_fraction\", 0.0)\n",
    "\n",
    "\n",
    "def _extract_attack_params(overrides: dict | None = None) -> dict:\n",
    "    params = {\n",
    "        \"type\": ATTACK_BASE.get(\"type\", \"pgd\"),\n",
    "        \"poison_rate\": ATTACK_BASE.get(\"poison_rate\", 0.0),\n",
    "        \"target_label\": ATTACK_BASE.get(\"target_label\", 0),\n",
    "        \"epsilon\": ATTACK_BASE.get(\"epsilon\", 0.03137255),\n",
    "        \"step_size\": ATTACK_BASE.get(\"step_size\", 0.00784314),\n",
    "        \"iters\": ATTACK_BASE.get(\"iters\", 10),\n",
    "        \"criterion\": ATTACK_BASE.get(\"criterion\", \"torch.nn.CrossEntropyLoss\"),\n",
    "    }\n",
    "    schedule = ATTACK_BASE.get(\"poison_rate_schedule\")\n",
    "    if schedule:\n",
    "        params[\"poison_rate_schedule\"] = schedule\n",
    "    if overrides:\n",
    "        params.update(overrides)\n",
    "    return params\n",
    "\n",
    "\n",
    "def _extract_surrogate_params(overrides: dict | None = None) -> dict:\n",
    "    params = {\n",
    "        \"pretrained\": SURROGATE_CFG.get(\"pretrained\", SURROGATE_BASE.get(\"pretrained\", True)),\n",
    "        \"finetune_epochs\": SURROGATE_CFG.get(\"num_epochs\", SURROGATE_BASE.get(\"finetune_epochs\", 0)),\n",
    "        \"lr\": SURROGATE_CFG.get(\"learning_rate\", SURROGATE_BASE.get(\"lr\", 1e-3)),\n",
    "        \"weight_decay\": SURROGATE_CFG.get(\"weight_decay\", SURROGATE_BASE.get(\"weight_decay\", 0.0)),\n",
    "        \"batch_size\": SURROGATE_CFG.get(\"batch_size\", SURROGATE_BASE.get(\"batch_size\", data_config.get(\"batch_size\", 96))),\n",
    "        \"client_id\": SURROGATE_CLIENT_ID,\n",
    "        \"pool_size\": SURROGATE_CFG.get(\"pool_size\", SURROGATE_BASE.get(\"pool_size\", 1)),\n",
    "        \"freeze_backbone\": SURROGATE_CFG.get(\"freeze_backbone\", SURROGATE_BASE.get(\"freeze_backbone\", False)),\n",
    "        \"augment\": SURROGATE_CFG.get(\"augment\", SURROGATE_BASE.get(\"augment\", False)),\n",
    "        \"early_stop_patience\": SURROGATE_CFG.get(\"early_stop_patience\", SURROGATE_BASE.get(\"early_stop_patience\", 0)),\n",
    "        \"num_classes\": SURROGATE_CFG.get(\"num_classes\", SURROGATE_BASE.get(\"num_classes\", model_config.get(\"kwargs\", {}).get(\"num_classes\", 10))),\n",
    "    }\n",
    "    if overrides:\n",
    "        params.update(overrides)\n",
    "    return params\n",
    "\n",
    "\n",
    "def build_attack_config(*, attack_overrides: dict | None = None, surrogate_overrides: dict | None = None, malicious_fraction: float | None = None, seed: int | None = None) -> dict:\n",
    "    cfg = {\n",
    "        \"seed\": seed if seed is not None else ATTACK_SEED,\n",
    "        \"malicious_fraction\": BASE_MALICIOUS_FRACTION if malicious_fraction is None else malicious_fraction,\n",
    "        \"attack\": _extract_attack_params(attack_overrides),\n",
    "        \"surrogate\": _extract_surrogate_params(surrogate_overrides),\n",
    "    }\n",
    "    for key in (\"start_round\",):\n",
    "        if key in ATTACK_RAW:\n",
    "            cfg[key] = ATTACK_RAW[key]\n",
    "    return cfg\n",
    "\n",
    "\n",
    "ATTACK_RECIPES = {\n",
    "    \"clean\": build_attack_config(malicious_fraction=0.0, attack_overrides={\"poison_rate\": 0.0}),\n",
    "    \"pgd_default\": build_attack_config(),\n",
    "    \"fgsm_default\": build_attack_config(attack_overrides={\"type\": \"fgsm\", \"iters\": 1}),\n",
    "    \"random_noise\": build_attack_config(attack_overrides={\"type\": \"random_noise\"}),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d289b829",
   "metadata": {},
   "source": [
    "## 11. Attack Implementations\n",
    "\n",
    "Notebook-native implementations of PGD, FGSM, and random noise that mirror the malicious client\u2019s expectations. Each function now supports a `targeted` flag so we can craft targeted poisons directly from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b6870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict\n",
    "\n",
    "AttackFn = Callable[..., torch.Tensor]\n",
    "\n",
    "\n",
    "def pgd_attack(\n",
    "    model: torch.nn.Module,\n",
    "    criterion: torch.nn.Module,\n",
    "    images: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    eps: float,\n",
    "    step_size: float,\n",
    "    iters: int,\n",
    "    targeted: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Projected Gradient Descent with an L-infinity constraint.\"\"\"\n",
    "    ori = images.clone().detach()\n",
    "    adv = ori.clone().detach()\n",
    "\n",
    "    for _ in range(iters):\n",
    "        adv.requires_grad_(True)\n",
    "        outputs = model(adv)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "\n",
    "        direction = -1 if targeted else 1\n",
    "        adv = adv + direction * step_size * adv.grad.sign()\n",
    "        eta = torch.clamp(adv - ori, min=-eps, max=eps)\n",
    "        adv = torch.clamp(ori + eta, 0, 1).detach()\n",
    "\n",
    "    return adv\n",
    "\n",
    "\n",
    "def fgsm_attack(\n",
    "    model: torch.nn.Module,\n",
    "    criterion: torch.nn.Module,\n",
    "    images: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    step_size: float,\n",
    "    targeted: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Single gradient-sign step.\"\"\"\n",
    "    adv = images.clone().detach().requires_grad_(True)\n",
    "    loss = criterion(model(adv), labels)\n",
    "\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    direction = -1 if targeted else 1\n",
    "    adv = adv + direction * step_size * adv.grad.sign()\n",
    "    return torch.clamp(adv, 0, 1).detach()\n",
    "\n",
    "\n",
    "def random_noise_attack(images: torch.Tensor, step_size: float) -> torch.Tensor:\n",
    "    \"\"\"Add random signed noise to the batch and clamp to valid bounds.\"\"\"\n",
    "    perturb = torch.randn_like(images).sign()\n",
    "    adv = images + step_size * perturb\n",
    "    return torch.clamp(adv, 0, 1).detach()\n",
    "\n",
    "\n",
    "ATTACK_FUNCTIONS: Dict[str, AttackFn] = {\n",
    "    \"pgd\": pgd_attack,\n",
    "    \"fgsm\": fgsm_attack,\n",
    "    \"random\": random_noise_attack,\n",
    "    \"random_noise\": random_noise_attack,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73b6552",
   "metadata": {},
   "source": [
    "## 12. Malicious Client Definition\n",
    "\n",
    "Helper to instantiate a `MaliciousClient` bound to our trained surrogate, including frozen backbone support, pooled loaders, and any extra hyper-parameter overrides pulled from the attack config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec05b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_malicious_client(\n",
    "    attack_config: dict,\n",
    "    *,\n",
    "    local_loader=None,\n",
    "    surrogate=None,\n",
    "    num_epochs: int = 0,\n",
    "    lr: float | None = None,\n",
    "):\n",
    "    \"\"\"Helper to instantiate a MaliciousClient bound to the trained surrogate.\"\"\"\n",
    "    if local_loader is None:\n",
    "        local_loader = get_surrogate_train_loader()\n",
    "    if surrogate is None:\n",
    "        surrogate = surrogate_model\n",
    "\n",
    "    client = MaliciousClient(\n",
    "        client_id=SURROGATE_CLIENT_ID,\n",
    "        local_data=local_loader,\n",
    "        device=get_device(),\n",
    "        num_epochs=num_epochs,\n",
    "        criterion=nn.CrossEntropyLoss().to(get_device()),\n",
    "        lr=lr if lr is not None else SURROGATE_CFG.get(\"local_lr\", 0.003),\n",
    "        attack_config=attack_config,\n",
    "    )\n",
    "    client.surrogate = surrogate.to(get_device())\n",
    "    client.surrogate.eval()\n",
    "    return client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa86a66",
   "metadata": {},
   "source": [
    "## 13. Attack Execution Helpers\n",
    "\n",
    "Utilities for selecting attack callables, crafting adversarial batches, and running both surrogate-level and federated-level attack sweeps. The helpers ensure each recipe is self-contained and reusable across experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975e70a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_attack_fn(name: str) -> AttackFn:\n",
    "    key = name.lower()\n",
    "    if key not in ATTACK_FUNCTIONS:\n",
    "        raise KeyError(f\"Unknown attack function '{name}'. Available: {sorted(ATTACK_FUNCTIONS)}\")\n",
    "    return ATTACK_FUNCTIONS[key]\n",
    "\n",
    "\n",
    "def _attach_attack_callable(cfg: dict) -> dict:\n",
    "    cfg_copy = deepcopy(cfg)\n",
    "    attack_params = cfg_copy.setdefault(\"attack\", {})\n",
    "    attack_type = attack_params.get(\"type\", \"pgd\")\n",
    "    if \"callable\" not in attack_params:\n",
    "        attack_params[\"callable\"] = select_attack_fn(attack_type)\n",
    "    return cfg_copy\n",
    "\n",
    "\n",
    "def craft_adversarial_batch(client: MaliciousClient, inputs: torch.Tensor, labels: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    attack_params = client.attack_params\n",
    "    attack_type = attack_params.get(\"type\", \"pgd\").lower()\n",
    "    target_label = int(attack_params.get(\"target_label\", 0))\n",
    "    target_labels = torch.full_like(labels, target_label)\n",
    "    targeted = bool(attack_params.get(\"targeted\", attack_params.get(\"target_label\") is not None))\n",
    "\n",
    "    if attack_type in {\"pgd\", \"fgsm\"}:\n",
    "        attack_fn = select_attack_fn(attack_type)\n",
    "        kwargs = {\n",
    "            \"model\": client.surrogate,\n",
    "            \"criterion\": client.attack_criterion,\n",
    "            \"images\": inputs,\n",
    "            \"labels\": target_labels,\n",
    "            \"step_size\": float(attack_params.get(\"step_size\", 0.00784314)),\n",
    "        }\n",
    "        kwargs[\"targeted\"] = targeted\n",
    "        if attack_type == \"pgd\":\n",
    "            kwargs[\"eps\"] = float(attack_params.get(\"epsilon\", 0.03137255))\n",
    "            kwargs[\"iters\"] = int(attack_params.get(\"iters\", 10))\n",
    "        adv = attack_fn(**kwargs)\n",
    "    else:\n",
    "        attack_fn = select_attack_fn(\"random_noise\")\n",
    "        adv = attack_fn(inputs, step_size=float(attack_params.get(\"step_size\", 0.00784314)))\n",
    "\n",
    "    return adv, target_labels\n",
    "\n",
    "\n",
    "def evaluate_surrogate_attack(recipe_name: str, *, batch_size: int = 32) -> dict:\n",
    "    attack_cfg = _attach_attack_callable(ATTACK_RECIPES[recipe_name])\n",
    "\n",
    "    client = make_malicious_client(\n",
    "        attack_cfg,\n",
    "        num_epochs=attack_cfg.get(\"surrogate\", {}).get(\"finetune_epochs\", 0),\n",
    "    )\n",
    "    client.surrogate = surrogate_model.to(get_device())\n",
    "    client.surrogate.eval()\n",
    "\n",
    "    loader = get_surrogate_test_loader()\n",
    "    inputs, labels = next(iter(loader))\n",
    "    inputs = inputs[:batch_size].to(get_device()).float()\n",
    "    labels = labels[:batch_size].to(get_device()).long()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        clean_logits = surrogate_model(inputs)\n",
    "        clean_preds = clean_logits.argmax(dim=1)\n",
    "        clean_acc = (clean_preds == labels).float().mean().item() * 100.0\n",
    "\n",
    "    adv_inputs, adv_labels = craft_adversarial_batch(client, inputs, labels)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        adv_logits = surrogate_model(adv_inputs)\n",
    "        adv_preds = adv_logits.argmax(dim=1)\n",
    "        asr = (adv_preds == adv_labels).float().mean().item() * 100.0\n",
    "\n",
    "    return {\n",
    "        \"recipe\": recipe_name,\n",
    "        \"clean_accuracy\": clean_acc,\n",
    "        \"attack_success_rate\": asr,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_attack_recipe_on_server(recipe_name: str, alg_name: str = \"FedAvg\", malicious_fraction: float | None = None) -> dict:\n",
    "    attack_cfg = _attach_attack_callable(ATTACK_RECIPES[recipe_name])\n",
    "    if malicious_fraction is not None:\n",
    "        attack_cfg = deepcopy(attack_cfg)\n",
    "        attack_cfg[\"malicious_fraction\"] = malicious_fraction\n",
    "    summary = run_one_algorithm(alg_name, attack_cfg=attack_cfg)\n",
    "    summary.update({\"recipe\": recipe_name, \"algorithm\": alg_name})\n",
    "    return summary\n",
    "\n",
    "\n",
    "def sweep_attacks_on_server(alg_name: str = \"FedAvg\", recipes: list[str] | None = None, malicious_fraction: float | None = None) -> dict:\n",
    "    recipes = recipes or list(ATTACK_RECIPES)\n",
    "    results = {}\n",
    "    for recipe in recipes:\n",
    "        if recipe == \"clean\":\n",
    "            results[recipe] = baseline_results.get(alg_name, {})\n",
    "            continue\n",
    "        results[recipe] = run_attack_recipe_on_server(\n",
    "            recipe,\n",
    "            alg_name=alg_name,\n",
    "            malicious_fraction=malicious_fraction,\n",
    "        )\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7656c3c",
   "metadata": {},
   "source": [
    "## 14. Surrogate Attack Experiments\n",
    "\n",
    "Evaluate how each attack recipe transfers on the surrogate alone: craft a batch of adversarial examples, measure clean vs. adversarial accuracy, and log the attack success rate before federated training enters the picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd413b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SURROGATE_ATTACK_RECIPES = [\"pgd_default\", \"fgsm_default\", \"random_noise\"]\n",
    "\n",
    "surrogate_attack_results = {\n",
    "    recipe: evaluate_surrogate_attack(recipe)\n",
    "    for recipe in SURROGATE_ATTACK_RECIPES\n",
    "}\n",
    "\n",
    "surrogate_attack_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist surrogate attack metrics\n",
    "\n",
    "Capture clean vs. adversarial accuracy for each recipe.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "sur_attack_path = ARTIFACT_DIR / 'module4_surrogate_attacks.json'\n",
    "with sur_attack_path.open('w') as f:\n",
    "    json.dump(surrogate_attack_results, f, indent=2)\n",
    "print(f'Saved surrogate attack metrics to {sur_attack_path.resolve()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4502a51",
   "metadata": {},
   "source": [
    "### Surrogate Attack Summary\n",
    "\n",
    "Visualise clean accuracy vs. attack success rate for each recipe to gauge transferability before federated training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e318dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def plot_surrogate_attack_results(results: dict[str, dict]) -> None:\n",
    "    df = pd.DataFrame(results).T\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    width = 0.35\n",
    "    x = range(len(df))\n",
    "    plt.bar([i - width / 2 for i in x], df['clean_accuracy'], width=width, label='Clean Acc')\n",
    "    plt.bar([i + width / 2 for i in x], df['attack_success_rate'], width=width, label='ASR')\n",
    "    plt.xticks(list(x), df.index, rotation=30)\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title('Surrogate attack outcomes')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_surrogate_attack_results(surrogate_attack_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce6d579",
   "metadata": {},
   "source": [
    "## 15. Federated Attack Sweeps\n",
    "\n",
    "Deploy poisoned clients inside the federated loop by reusing the same recipes, then compare clean baselines with attacked runs across algorithms to see how poisoning degrades global performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4637060",
   "metadata": {},
   "outputs": [],
   "source": [
    "FED_ATTACK_RECIPES = [\"clean\", \"pgd_default\"]\n",
    "\n",
    "federated_attack_results = sweep_attacks_on_server(\n",
    "    alg_name=\"FedAvg\",\n",
    "    recipes=FED_ATTACK_RECIPES,\n",
    ")\n",
    "\n",
    "federated_attack_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist federated attack metrics\n",
    "\n",
    "Record clean vs. PGD-poisoned outcomes for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "fed_attack_path = ARTIFACT_DIR / 'module4_federated_attacks.json'\n",
    "with fed_attack_path.open('w') as f:\n",
    "    json.dump(federated_attack_results, f, indent=2)\n",
    "print(f'Saved federated attack metrics to {fed_attack_path.resolve()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federated attack sanity check\n",
    "\n",
    "Confirm the attack run differs from the clean baseline so we notice configuration mistakes early.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "clean_acc = baseline_results.get('FedAvg', {}).get('final_accuracy')\n",
    "attack_acc = federated_attack_results.get('pgd_default', {}).get('final_accuracy')\n",
    "if clean_acc is not None and attack_acc is not None:\n",
    "    delta = clean_acc - attack_acc\n",
    "    print(f'FedAvg clean accuracy: {clean_acc:.2f}%  |  attacked: {attack_acc:.2f}%  |  drop: {delta:.2f} pts')\n",
    "else:\n",
    "    print('Run the baseline and attack cells before executing this check.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a435b3d0",
   "metadata": {},
   "source": [
    "### Federated Attack Impact\n",
    "\n",
    "Compare final accuracies between clean and attacked runs for the chosen algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50733747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resolve_accuracy(summary: dict | None) -> float:\n",
    "    if not summary:\n",
    "        return 0.0\n",
    "    value = summary.get('final_accuracy')\n",
    "    return float(value) if value is not None else 0.0\n",
    "\n",
    "def plot_federated_attack_results(clean: dict, attacked: dict) -> None:\n",
    "    algs = sorted({*clean.keys(), *attacked.keys()})\n",
    "    if not algs:\n",
    "        print('No federated results available to plot yet.')\n",
    "        return\n",
    "    clean_acc = [_resolve_accuracy(clean.get(alg)) for alg in algs]\n",
    "    attack_acc = [_resolve_accuracy(attacked.get(alg)) for alg in algs]\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    width = 0.35\n",
    "    positions = list(range(len(algs)))\n",
    "    plt.bar([i - width / 2 for i in positions], clean_acc, width=width, label='Clean')\n",
    "    plt.bar([i + width / 2 for i in positions], attack_acc, width=width, label='Attacked')\n",
    "    plt.xticks(positions, algs, rotation=30)\n",
    "    plt.ylabel('Final accuracy (%)')\n",
    "    plt.title('Federated accuracy with and without attack')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_federated_attack_results(baseline_results, federated_attack_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd6fa47",
   "metadata": {},
   "source": [
    "## 16. Malicious Fraction Sweep\n",
    "\n",
    "Explore how varying the proportion of malicious clients changes the global outcome. We reuse the sweep helper while overriding the malicious fraction for each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af07fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_grid = [0.0, 0.05, 0.1, 0.2]\n",
    "fraction_sweep_results = {}\n",
    "for frac in malicious_grid:\n",
    "    mf = frac if frac > 0 else None\n",
    "    fraction_sweep_results[frac] = sweep_attacks_on_server(\n",
    "        alg_name=\"FedAvg\",\n",
    "        recipes=[\"clean\", \"pgd_default\"],\n",
    "        malicious_fraction=mf,\n",
    "    )\n",
    "\n",
    "comparison_rows = []\n",
    "for frac, results in fraction_sweep_results.items():\n",
    "    attack_summary = results.get(\"pgd_default\", {})\n",
    "    comparison_rows.append({\n",
    "        \"malicious_fraction\": frac,\n",
    "        \"final_loss\": attack_summary.get(\"final_loss\"),\n",
    "        \"final_accuracy\": attack_summary.get(\"final_accuracy\"),\n",
    "    })\n",
    "\n",
    "comparison_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist malicious-fraction sweep\n",
    "\n",
    "Store the summary table for different malicious client ratios.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "fraction_path = ARTIFACT_DIR / 'module4_fraction_sweep.json'\n",
    "with fraction_path.open('w') as f:\n",
    "    json.dump(comparison_rows, f, indent=2)\n",
    "print(f'Saved sweep results to {fraction_path.resolve()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a322d74",
   "metadata": {},
   "source": [
    "### Malicious Fraction vs. Accuracy\n",
    "\n",
    "Plot the final accuracy achieved by PGD attacks as we vary the proportion of malicious clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024b3f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fraction_sweep(results: dict[float, dict] | None) -> None:\n",
    "    if not results:\n",
    "        raise ValueError(\"No fraction sweep results available to plot.\")\n",
    "    fractions = sorted(results.keys())\n",
    "    accuracies = [results[f].get('pgd_default', {}).get('final_accuracy') for f in fractions]\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fractions, accuracies, marker='o')\n",
    "    plt.xlabel('Malicious fraction')\n",
    "    plt.ylabel('Final accuracy (%)')\n",
    "    plt.title('Impact of malicious fraction on FedAvg (PGD)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "_fraction_results = globals().get('fraction_sweep_results')\n",
    "if not _fraction_results:\n",
    "    print(\"fraction_sweep_results missing; run the sweep cell above to populate it before plotting.\")\n",
    "else:\n",
    "    plot_fraction_sweep(_fraction_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}