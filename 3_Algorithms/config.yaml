data_config:
  dataset_path: "./data"
  dataset_name: "MNIST"
  non_iid_per: 0

global_config:
  seed: 42
  device: "cuda"

model_config:
  module: "model"
  name: "AlexNet"
  kwargs:
    num_classes: 10

algorithms:
  FedAvg:
    fed_config:
      fraction_clients: 0.1
      num_clients: 100
      num_rounds: 5
      num_epochs: 5
      batch_size: 64
      global_stepsize: 0.1
      local_stepsize: 0.01
      criterion: "torch.nn.CrossEntropyLoss"
    optim_config: {}

  Scaffold:
    fed_config:
      fraction_clients: 0.1
      num_clients: 100
      num_rounds: 5
      num_epochs: 5
      batch_size: 64
      global_stepsize: 1.0  # SCAFFOLD uses larger server LR
      local_stepsize: 0.01
      criterion: "torch.nn.CrossEntropyLoss"
    optim_config:
      c_init: 0.0  # Initialize control variates to zero

  FedAdam:
    fed_config:
      fraction_clients: 0.1
      num_clients: 100
      num_rounds: 5
      num_epochs: 5
      batch_size: 64
      global_stepsize: 0.01  # Adaptive methods need smaller server LR
      local_stepsize: 0.01
      criterion: "torch.nn.CrossEntropyLoss"
    optim_config:
      beta1: 0.9
      beta2: 0.99
      epsilon: 0.000001

  FedAdagrad:
    fed_config:
      fraction_clients: 0.1
      num_clients: 100
      num_rounds: 5
      num_epochs: 5
      batch_size: 64
      global_stepsize: 0.01  # Adaptive methods need smaller server LR
      local_stepsize: 0.01
      criterion: "torch.nn.CrossEntropyLoss"
    optim_config:
      epsilon: 0.000001

  FedYogi:
    fed_config:
      fraction_clients: 0.1
      num_clients: 100
      num_rounds: 5
      num_epochs: 5
      batch_size: 64
      global_stepsize: 0.01  # Adaptive methods need smaller server LR
      local_stepsize: 0.01
      criterion: "torch.nn.CrossEntropyLoss"
    optim_config:
      beta1: 0.9
      beta2: 0.99
      epsilon: 0.000001
