{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lYjvZ3vMyaTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z1EgzA_ylmF",
        "outputId": "6fcc11e5-28bf-4f8c-f691-39e37ed32584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSc48na9xwU1"
      },
      "outputs": [],
      "source": [
        "import os, math, random, logging\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool  = nn.MaxPool2d(2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        return self.fc2(x)\n",
        "def download_mnist(root=\"./data\"):\n",
        "\n",
        "    _ = datasets.MNIST(root=root, train=True,  download=True)\n",
        "    _ = datasets.MNIST(root=root, train=False, download=True)\n",
        "    print(f\"MNIST downloaded to: {os.path.abspath(root)}\")\n",
        "def mnist_transforms():\n",
        "\n",
        "    return transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "def load_mnist_with_transforms(root=\"./data\", transform=None):\n",
        "\n",
        "    if transform is None:\n",
        "        transform = mnist_transforms()\n",
        "    train_ds = datasets.MNIST(root=root, train=True,  download=False, transform=transform)\n",
        "    test_ds  = datasets.MNIST(root=root, train=False, download=False, transform=transform)\n",
        "    return train_ds, test_ds\n",
        "def _iid_indices(n_items, num_clients, seed=42):\n",
        "    # print(\"iid\")\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = np.arange(n_items)\n",
        "    rng.shuffle(idx)\n",
        "    splits = np.array_split(idx, num_clients)\n",
        "    return [np.array(s, dtype=int) for s in splits]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Non-IID Data in Federated Learning\n",
        "\n",
        "In real-world federated learning, client data is rarely **IID (independent and identically distributed)**.  \n",
        "For example:\n",
        "- A phone user may only write certain digits, so their dataset is biased.  \n",
        "- A hospital may serve a specific demographic, creating skewed medical records.  \n",
        "\n",
        "To simulate this, we use **non-IID splits** of MNIST.\n",
        "\n",
        "---\n",
        "\n",
        "## Dirichlet Distribution for Data Partitioning\n",
        "\n",
        "We use the **Dirichlet distribution** to control how labels are assigned to clients:\n",
        "\n",
        "- The Dirichlet distribution is a **probability distribution over probability vectors**.  \n",
        "- Each vector element represents the proportion of a class assigned to a client.  \n",
        "- The parameter **α (alpha)** controls how \"skewed\" the distribution is:\n",
        "\n",
        "  - **High α (e.g., α = 10)** → more uniform → data is closer to IID.  \n",
        "  - **Low α (e.g., α = 0.1)** → very skewed → each client may only see a few labels.  \n",
        "\n",
        "---\n",
        "\n",
        "### Example\n",
        "\n",
        "- Suppose we have 10 clients and 10 digit classes.  \n",
        "- For each digit, we sample a probability vector from Dirichlet(α).  \n",
        "- This vector decides how much of that digit’s data goes to each client.  \n",
        "- Repeating for all digits creates **client datasets with unique label distributions**.  "
      ],
      "metadata": {
        "id": "sMDJSA1DDdDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def _dirichlet_label_skew_indices(targets, num_clients, alpha=0.5, seed=42):\n",
        "\n",
        "    # print(\"skew\")\n",
        "    rng = np.random.default_rng(seed)\n",
        "    targets = np.asarray(targets)\n",
        "    classes = np.unique(targets)\n",
        "    client_indices = [[] for _ in range(num_clients)]\n",
        "\n",
        "    for c in classes:\n",
        "        c_idx = np.where(targets == c)[0]\n",
        "        rng.shuffle(c_idx)\n",
        "\n",
        "        props = rng.dirichlet([alpha] * num_clients)\n",
        "\n",
        "        counts = (len(c_idx) * props).astype(int)\n",
        "\n",
        "        while counts.sum() < len(c_idx):\n",
        "            counts[np.argmax(props)] += 1\n",
        "\n",
        "        start = 0\n",
        "        for i, cnt in enumerate(counts):\n",
        "            if cnt > 0:\n",
        "                client_indices[i].extend(c_idx[start:start+cnt])\n",
        "                start += cnt\n",
        "\n",
        "\n",
        "    for i in range(num_clients):\n",
        "        client_indices[i] = np.array(client_indices[i], dtype=int)\n",
        "        rng.shuffle(client_indices[i])\n",
        "    return client_indices\n"
      ],
      "metadata": {
        "id": "WEK1bNezDa5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load client data\n",
        " Create DataLoader objects for each client.\n",
        " Each loader serves local batches of training data.\n",
        " Supports IID or non-IID distributions."
      ],
      "metadata": {
        "id": "U74oPOJuAq9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def make_client_loaders(\n",
        "    train_ds,\n",
        "    test_ds,\n",
        "    num_clients=10,\n",
        "    batch_size=64,\n",
        "    non_iid_per=0.0,\n",
        "    seed=42,\n",
        "):\n",
        "\n",
        "    if non_iid_per <= 1e-8:\n",
        "        client_idxs = _iid_indices(len(train_ds), num_clients, seed=seed)\n",
        "    else:\n",
        "\n",
        "        alpha = max(0.01, 1.0 - 0.99 * non_iid_per)\n",
        "        targets = train_ds.targets if hasattr(train_ds, \"targets\") else train_ds.labels\n",
        "        client_idxs = _dirichlet_label_skew_indices(targets, num_clients, alpha=alpha, seed=seed)\n",
        "\n",
        "    local_loaders = [\n",
        "        DataLoader(Subset(train_ds, idxs), batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "        for idxs in client_idxs\n",
        "    ]\n",
        "    test_loader = DataLoader(test_ds, batch_size=512, shuffle=False, drop_last=False)\n",
        "    return local_loaders, test_loader"
      ],
      "metadata": {
        "id": "M3JX-JIgAoS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuTz4veEs5gx"
      },
      "source": [
        "#Client\n",
        "## Holds local data.\n",
        "##Performs local training for a few epochs on its copy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdWuIMZhxwU3"
      },
      "outputs": [],
      "source": [
        "class Client():\n",
        "\n",
        "    def __init__(self, client_id, local_data, device, num_epochs, criterion, lr):\n",
        "        self.id = client_id\n",
        "        self.data = local_data\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.num_epochs = num_epochs\n",
        "        self.lr = lr\n",
        "        self.criterion = criterion\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "\n",
        "    def client_update(self):\n",
        "        # print(\"update\")\n",
        "\n",
        "\n",
        "        self.y = deepcopy(self.x)\n",
        "        self.y.to(self.device)\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "\n",
        "            for inputs,labels in self.data:\n",
        "              inputs, labels = inputs.float().to(self.device), labels.long().to(self.device)\n",
        "              output = self.y(inputs)\n",
        "              loss = self.criterion(output, labels)\n",
        "              grads = torch.autograd.grad(loss,self.y.parameters())\n",
        "\n",
        "              with torch.no_grad():\n",
        "                  for param,grad in zip(self.y.parameters(),grads):\n",
        "                      param.data = param.data - self.lr * grad.data\n",
        "\n",
        "              if self.device == \"cuda\": torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate function\n",
        "## Evaluate a model on the test dataset.\n",
        "## Returns average loss and accuracy across all test samples."
      ],
      "metadata": {
        "id": "eTHAdDkhA_zF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_zmWJo9xwU3"
      },
      "outputs": [],
      "source": [
        "def evaluate_fn(test_loader, model, criterion, device):\n",
        "    # print(\"eval\")\n",
        "    model.eval()\n",
        "    n, total_loss, correct = 0, 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x = x.to(device, dtype=torch.float)\n",
        "            y = y.to(device, dtype=torch.long)\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            n += x.size(0)\n",
        "    return (total_loss / max(1, n)), (100.0 * correct / max(1, n))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Client Sampling\n",
        "## Randomly sample a fraction of clients each round.\n",
        "## Simulates partial participation in federated training."
      ],
      "metadata": {
        "id": "FNjy8PjuBUMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sample_clients(num_clients, fraction, rng):\n",
        "    k = max(1, int(math.ceil(fraction * num_clients)))\n",
        "    return sorted(rng.choice(np.arange(num_clients), size=k, replace=False).tolist())\n"
      ],
      "metadata": {
        "id": "M9ejG82eBOg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Federated Averaging\n",
        "## Aggregate model parameters from selected clients.\n",
        "## Compute average weights and update the global model.\n"
      ],
      "metadata": {
        "id": "p-GWDch3Bb1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def fedavg_aggregate(global_model, client_models, device):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # init accumulators\n",
        "        acc = [torch.zeros_like(p, device=device) for p in global_model.parameters()]\n",
        "        for cm in client_models:\n",
        "            for a, p in zip(acc, cm.parameters()):\n",
        "                a.add_(p.to(device))\n",
        "        for gp, a in zip(global_model.parameters(), acc):\n",
        "            gp.copy_(a / len(client_models))\n"
      ],
      "metadata": {
        "id": "Lvhr183bBQfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training\n",
        "## Initialize global model.\n",
        "## Each round: sample clients, broadcast model, perform local training.\n",
        "## Aggregate updates with FedAvg.\n",
        "## Evaluate global model on test data."
      ],
      "metadata": {
        "id": "ez9vTaExBrpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_fedavg(\n",
        "    model_ctor,\n",
        "    client_loaders,\n",
        "    test_loader,\n",
        "    *,\n",
        "    device,\n",
        "    rounds=20,\n",
        "    local_epochs=1,\n",
        "    fraction=0.1,\n",
        "    local_lr=0.01,\n",
        "    criterion=None,\n",
        "    seed=42,\n",
        "    log_level=logging.INFO,\n",
        "):\n",
        "\n",
        "    # print(\"train\")\n",
        "    logging.basicConfig(level=log_level, format=\"%(message)s\")\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # global model\n",
        "    global_model = model_ctor().to(device)\n",
        "    if criterion is None:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # wrap clients\n",
        "    clients = [\n",
        "        Client(i, ld, device=device, num_epochs=local_epochs, criterion=criterion, lr=local_lr)\n",
        "        for i, ld in enumerate(client_loaders)\n",
        "    ]\n",
        "\n",
        "    history = {\"round\": [], \"loss\": [], \"acc\": []}\n",
        "\n",
        "    for r in range(1, rounds + 1):\n",
        "        # print(\"round\")\n",
        "        # sample & broadcast\n",
        "        ids = sample_clients(len(clients), fraction, rng)\n",
        "        for i in ids:\n",
        "            clients[i].x = global_model  # reference (they deepcopy inside)\n",
        "\n",
        "        # local updates\n",
        "        for i in ids:\n",
        "            clients[i].client_update()\n",
        "\n",
        "        # aggregate\n",
        "        fedavg_aggregate(global_model, [clients[i].y for i in ids], device=device)\n",
        "\n",
        "        # evaluate\n",
        "        test_loss, test_acc = evaluate_fn(test_loader, global_model, criterion, device)\n",
        "        history[\"round\"].append(r); history[\"loss\"].append(test_loss); history[\"acc\"].append(test_acc)\n",
        "        logging.info(f\"Round {r:03d} | test loss: {test_loss:.4f} | test acc: {test_acc:.2f}%\")\n",
        "        print(f\"Round {r:03d} | test loss: {test_loss:.4f} | test acc: {test_acc:.2f}%\")\n",
        "\n",
        "    return global_model, history"
      ],
      "metadata": {
        "id": "uln6djjEBSN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run Experiment\n",
        "## Set hyperparameters (clients, batch size, rounds, etc.).\n",
        "## Download MNIST and prepare client data loaders.\n",
        "## Train the model using FedAvg and log performance per round."
      ],
      "metadata": {
        "id": "MAL90PGyB25l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNhZvlRsxwU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0bcbee3-f77f-4703-cac6-051849317614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST downloaded to: /content/data\n",
            "Round 001 | test loss: 1.4690 | test acc: 57.84%\n",
            "Round 002 | test loss: 0.5028 | test acc: 83.99%\n",
            "Round 003 | test loss: 0.7381 | test acc: 75.21%\n",
            "Round 004 | test loss: 0.6713 | test acc: 78.35%\n",
            "Round 005 | test loss: 0.5078 | test acc: 81.43%\n",
            "Round 006 | test loss: 1.0226 | test acc: 68.45%\n",
            "Round 007 | test loss: 0.3826 | test acc: 88.29%\n",
            "Round 008 | test loss: 0.3769 | test acc: 87.51%\n",
            "Round 009 | test loss: 0.3534 | test acc: 88.72%\n",
            "Round 010 | test loss: 0.3223 | test acc: 90.00%\n",
            "Round 011 | test loss: 0.2165 | test acc: 93.60%\n",
            "Round 012 | test loss: 0.2580 | test acc: 91.79%\n",
            "Round 013 | test loss: 0.2593 | test acc: 92.04%\n",
            "Round 014 | test loss: 0.2357 | test acc: 92.29%\n",
            "Round 015 | test loss: 0.2653 | test acc: 91.88%\n",
            "Round 016 | test loss: 0.2022 | test acc: 93.69%\n",
            "Round 017 | test loss: 0.2740 | test acc: 90.61%\n",
            "Round 018 | test loss: 0.1894 | test acc: 94.11%\n",
            "Round 019 | test loss: 0.1756 | test acc: 94.54%\n",
            "Round 020 | test loss: 0.2257 | test acc: 92.95%\n"
          ]
        }
      ],
      "source": [
        "seed = 42\n",
        "set_seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data_root = \"./data\"\n",
        "num_clients = 10\n",
        "batch_size = 64\n",
        "non_iid_per = 0.5     # 0 = IID, 1 = very non-IID\n",
        "rounds = 20\n",
        "local_epochs = 1\n",
        "fraction = 0.2         # 20% clients per round\n",
        "local_lr = 0.01\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Data\n",
        "download_mnist(data_root)\n",
        "train_ds, test_ds = load_mnist_with_transforms(data_root)\n",
        "client_loaders, test_loader = make_client_loaders(\n",
        "    train_ds, test_ds, num_clients=num_clients,\n",
        "    batch_size=batch_size, non_iid_per=non_iid_per, seed=seed\n",
        ")\n",
        "\n",
        "# Train\n",
        "global_model, history = train_fedavg(\n",
        "    Net, client_loaders, test_loader,\n",
        "    device=device, rounds=rounds, local_epochs=local_epochs,\n",
        "    fraction=fraction, local_lr=local_lr, criterion=criterion, seed=seed\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zNm3JlfkGroT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}